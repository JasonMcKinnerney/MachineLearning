{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 9\n",
    "This assignment covers the overall concepts of Neural Networks and Convolutional Neural Networks.  \n",
    "\n",
    "\n",
    "## Install Tensorflow \n",
    "\n",
    "If ```import tensorflow as tf```  gives Module not found error then it means tensorflow is not installed.\n",
    "\n",
    "conda: ```conda create -n tf tensorflow  \n",
    "conda activate tf```\n",
    "\n",
    "pip: ```pip install tensorflow```\n",
    "\n",
    "## Tutorials\n",
    "\n",
    "* [Tensorflow Quickstart](https://www.tensorflow.org/tutorials/quickstart/beginner)\n",
    "* [MNIST Basic Image Classification w Keras](https://www.tensorflow.org/tutorials/keras/classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Keep the following in mind for all notebooks you develop:\n",
    "* Structure your notebook. \n",
    "* Use headings with meaningful levels in Markdown cells, and explain the questions each piece of code is to answer or the reason it is there.\n",
    "* Make sure your notebook can always be rerun from top to bottom.\n",
    "* Please start working on this assignment as soon as possible. If you are a beginner in Python this might take a long time. One of the objectives of this assignment is to help you learn python and scikit-learn package. \n",
    "* See [README.md](../README.md) for homework submission instructions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling w Tensorflow \n",
    "\n",
    "**DO NOT ERASE MARKDOWN CELLS AND INSTRUCTIONS IN YOUR HW submission**\n",
    "\n",
    "  * **Q** - QUESTION\n",
    "  * **A** - Where to input your answer\n",
    "\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided HW9 data MNIST 70,000 images w labels in in 2 files: \n",
    "* [HW9_train](HW9_train.csv) contains 42000 28 x 28 training images, one image per line represented as grayscale pixel values for 784 pixels. \n",
    "* [HW9_test](HW9_test.csv) contains 28000 28 x 28 training images, one image per line represented as grayscale pixel values for 784 pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1** Get training data from the dataframe\n",
    "1. Load HW9_train.csv into data frame and print the info() of the dataframe to get overall idea about the dataset\n",
    "2. Create train data and validation data from the dataframe by random selection \n",
    "   * 38000 is the model train data \n",
    "   * 4000 is the model validation data \n",
    "6. Then generate label files for train and validation data by droping the label column from the dataframes.\n",
    "   * use dataframe ``pop function`` \n",
    "\n",
    "**A1** Replace ??? with code in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42000 entries, 0 to 41999\n",
      "Columns: 785 entries, label to pixel783\n",
      "dtypes: int64(785)\n",
      "memory usage: 251.5 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('HW9_train.csv')\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    0\n",
      "2    1\n",
      "3    4\n",
      "4    0\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_data = data.head(38000)\n",
    "val_data = data.tail(4000)\n",
    "\n",
    "train_labels = train_data.pop('label')\n",
    "val_labels = val_data.pop('label')\n",
    "\n",
    "print(train_labels[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Dataset\n",
    "\n",
    "We are using [Tensorflow](https://www.tensorflow.org/tutorials/quickstart/beginner) to train an image classifier model. Tensorflow has it's own api for creating a data pipeline which makes it easier to feed the data into a Tensorflow model. \n",
    "* To use this api the data needs to be loaded into a Tensorflow data object. \n",
    "* Tensorflow [Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2** Create Data pipeline using the tensorflow api from the dataframes and labels\n",
    "1. Create tensorflow dataset from train dataframe\n",
    "2. Create tensorflow dataset from validation dataframe\n",
    "\n",
    "**A2** Replace ??? with code in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((784,), ()), types: (tf.int64, tf.int64)>\n",
      "<TensorSliceDataset shapes: ((784,), ()), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "tf_train_data = tf.data.Dataset.from_tensor_slices((train_data.values, train_labels.values))\n",
    "tf_val_data = tf.data.Dataset.from_tensor_slices((val_data.values, val_labels.values))\n",
    "\n",
    "print(tf_train_data)\n",
    "print(tf_val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Visualization and Data Scaling\n",
    "\n",
    "Before going on to form the data pipeline, lets have a look at some of the images in the dataset and visualise them with their labels. Pixels for the images have been flattened into a 2D array, so use a reshape function to put the images into a 28x28 shape that is more human friendly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3** Lets plot our numeric data as images\n",
    "1. Pick 10 data from the tensorflow data bundle\n",
    "2. Convert the 1D series into numpy array and Reshape the numpy array into 28X28 2D array\n",
    "3. Use matplotlib ``imshow()`` function for plotting the 2D numpy array as image\n",
    "\n",
    "**A3** Replace ??? with code in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "i = 0\n",
    "\n",
    "for image, label in tf_train_data.take(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.imshow(image.numpy().reshape(28,28), cmap='gray')\n",
    "    plt.xlabel(label.numpy())\n",
    "\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets reshape and normalize the data for the training: \n",
    "1. **Reshape:** The images are currently formatted as a flat list of pixels. CNNs need the data to be reshaped into a 3D matrix which in this case is a 28x28 matrix. I also need to specify the channels. As these images are greyscale I only need the one. The shape of the data then is (28, 28, 1).\n",
    "2. **Scale:** Models seem to perform better when numeric features are put onto a 0-1 scale. As all the features are numeric and on a 0-255 scale it is easy to put the data onto this scale by simply dividing the features by 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4** Complete the preprocess_image() function so it correctly reshapes and normalizes the data: \n",
    "* Reshape every image data into (28, 28, 1) matrix. \n",
    "* Normalize all pixel values by dividing 255. It will make all values between 0-1\n",
    "\n",
    "**A4** Replace ??? with code in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ParallelMapDataset shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n",
      "<ParallelMapDataset shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_image(image, label):\n",
    "    image = tf.reshape(image, [28, 28, 1])\n",
    "    image = tf.cast(image, tf.float32) / 255.\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "tf_train_data = tf_train_data.map(\n",
    "    preprocess_image, \n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "\n",
    "tf_val_data = tf_val_data.map(\n",
    "    preprocess_image, \n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "\n",
    "print(tf_train_data)\n",
    "print(tf_val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Next, we form a pipeline to feed the images into the model.  Now the data is preprocessed there are few configurations that can be made to the pipeline. Here's a list of those configurations:\n",
    "1. **Shuffle:** To make sure the model doesn't pick up anything from the order of the rows in the dataset the top 100 rows of data will be shuffled per training step. \n",
    "2. **Batch:** This is quite a large dataset in terms of both rows and columns. To ensure the processor doesn't get overloaded the data will be fed in 32 images at a time.\n",
    "3. **Prefetch:** The speed up the training the pipeline can be set to fetch the next batch of data while the current batch of data is in training. When the current step ends there is no time wasted loading the next batch as it is ready to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5** Create data pipeline on the training set, and validation set\n",
    "\n",
    "1. complete the pipeline() function\n",
    "2. shuffle the train and validation dataset 100 times\n",
    "3. Make input batches with batch size 32\n",
    "\n",
    "**A5** Replace ??? with code in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>\n",
      "<PrefetchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "def pipeline(tf_data):\n",
    "    tf_data = tf_data.shuffle(100)\n",
    "    tf_data = tf_data.batch(32)\n",
    "    tf_data = tf_data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return tf_data\n",
    "\n",
    "tf_train_data = pipeline(tf_train_data)\n",
    "tf_val_data = pipeline(tf_val_data)\n",
    "\n",
    "print(tf_train_data)\n",
    "print(tf_val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition \n",
    "\n",
    "We use Tensorflow to define the model that is LeNet inspired [Keras example](https://www.kaggle.com/niranjanjagannath/lenet-5-architecture-for-mnist-using-tensorflow). \n",
    "1. All average pooling layers have been replaced with max pooling layers\n",
    "2. The input shape is 28x28, not 32x32. The first convolutional layer uses \"same\" padding to ensure the data is in the same shape as it is in the paper by the time it hits the first pooling layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6** Create CNN Model on the training set, and evaluate\n",
    "1. The model will be of 2 ConV net and followed by MaxPooling after each of them  \n",
    "* Layer 1 is a convolutional layer (ConV net): Kernel is 5X5 and number of kernel is 6\n",
    "  * Maxpool after 1st ConV net: 2X2\n",
    "* Layer 2 is a convolutional layer (ConV net): Kernel is 5X5 and number of kernel is 16\n",
    "  * Maxpool after 2nd ConV net: 2X2\n",
    "  * Flatten this  output \n",
    "3. Layer 3 is a dense layer with 120 output units using ``relu`` as activation function\n",
    "4. Layer 4 is a dense layer with 84 output units using ``relu`` as activation function\n",
    "5. Layer 5 (last layer) is a Dense Layer that is using ``softmax`` as an activation function \n",
    "\n",
    "**A6** Replace ??? with code in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(5,5, activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(5, 5, activation='relu', padding='valid'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='relu'),\n",
    "    tf.keras.layers.Dense(84, activation='relu'),\n",
    "    \n",
    "    tf.keras.layers.Dense(5, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7** Lets set an optimizer for the model training.  Use Tensorflow [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) as the optimizer with the learning rate of 0.001 and the  loss function of [sparse categorical crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy). We will keep accuracy as the evaluation measure.  Print model summary and see the number of trainable weights\n",
    "\n",
    "**A7** Replace ??? with code in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 5)         130       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 5)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 10, 10, 5)         630       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 5)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 125)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               15120     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 425       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,469\n",
      "Trainable params: 26,469\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimiser = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimiser, \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have loaded the training data, and compiled the model, now it is time to train it. The validation dataset has been included here so that the model validates its accuracy after each step by making predictions against the validation dataset.  Each epoch takes under a minute to complete so this may take a few minutes to run depending on your machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the CNN and Plot the Accuracy\n",
    "**Q8** Start the training using fit function\n",
    "1. Pass train data inside fit function\n",
    "2. Provide validation data inside fit function\n",
    "3. Define number of epochs(For faster training set it equal to 10)\n",
    "\n",
    "**A8** Replace ??? with code in the code cell below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Received a label value of 9 which is outside the valid range of [0, 5).  Label values: 2 2 6 6 0 3 9 6 3 0 5 9 3 7 0 1 9 8 0 2 6 7 0 2 7 9 9 4 1 9 2 2\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n (defined at C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\backend.py:5113)\n]] [Op:__inference_train_function_897]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:\nIn[0] sparse_categorical_crossentropy/Reshape_1 (defined at C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\backend.py:5109)\t\nIn[1] sparse_categorical_crossentropy/Reshape (defined at C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\backend.py:3561)\n\nOperation defined at: (most recent call last)\n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\asyncio\\events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n>>>     lambda f: self._run_callback(functools.partial(callback, future))\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n>>>     ret = callback()\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n>>>     self.ctx_run(self.run)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n>>>     yielded = self.gen.send(value)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n>>>     yield gen.maybe_future(dispatch(*args))\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n>>>     yield gen.maybe_future(handler(stream, idents, msg))\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 543, in execute_request\n>>>     self.do_execute(\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2894, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3165, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3357, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-12-c1fd1d2e0c5c>\", line 1, in <module>\n>>>     train_log = model.fit(\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 809, in train_step\n>>>     loss = self.compiled_loss(\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n>>>     loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n>>>     losses = call_fn(y_true, y_pred)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 245, in call\n>>>     return ag_fn(y_true, y_pred, **self._fn_kwargs)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 1737, in sparse_categorical_crossentropy\n>>>     return backend.sparse_categorical_crossentropy(\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5113, in sparse_categorical_crossentropy\n>>>     res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-d28874f110c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m train_log = model.fit(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mtf_train_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf_val_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  Received a label value of 9 which is outside the valid range of [0, 5).  Label values: 2 2 6 6 0 3 9 6 3 0 5 9 3 7 0 1 9 8 0 2 6 7 0 2 7 9 9 4 1 9 2 2\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n (defined at C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\backend.py:5113)\n]] [Op:__inference_train_function_897]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:\nIn[0] sparse_categorical_crossentropy/Reshape_1 (defined at C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\backend.py:5109)\t\nIn[1] sparse_categorical_crossentropy/Reshape (defined at C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\backend.py:3561)\n\nOperation defined at: (most recent call last)\n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\asyncio\\events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n>>>     lambda f: self._run_callback(functools.partial(callback, future))\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n>>>     ret = callback()\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n>>>     self.ctx_run(self.run)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n>>>     yielded = self.gen.send(value)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n>>>     yield gen.maybe_future(dispatch(*args))\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n>>>     yield gen.maybe_future(handler(stream, idents, msg))\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 543, in execute_request\n>>>     self.do_execute(\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2894, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3165, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3357, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-12-c1fd1d2e0c5c>\", line 1, in <module>\n>>>     train_log = model.fit(\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 809, in train_step\n>>>     loss = self.compiled_loss(\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n>>>     loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n>>>     losses = call_fn(y_true, y_pred)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 245, in call\n>>>     return ag_fn(y_true, y_pred, **self._fn_kwargs)\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 1737, in sparse_categorical_crossentropy\n>>>     return backend.sparse_categorical_crossentropy(\n>>> \n>>>   File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5113, in sparse_categorical_crossentropy\n>>>     res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n>>> "
     ]
    }
   ],
   "source": [
    "train_log = model.fit(\n",
    "    tf_train_data,\n",
    "    validation_data=tf_val_data,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9** Create plot on the training set, and validation set. Use ``history`` attribute from the above train_log for creating accuracy curve.\n",
    "\n",
    "\n",
    "**A9** Replace ??? with code in the code cell below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-f404d65d5086>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-16-f404d65d5086>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    plt.plot(train_log.history['accuracy', label='accuracy')\u001b[0m\n\u001b[1;37m                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "plt.plot(train_log.history['accuracy', label='accuracy')\n",
    "plt.plot(train_log.history['val_accuracy', label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "print('Training accuracy: %f' % train_log.history['accuracy'][-1])\n",
    "print('Validation accuracy: %f' % train_log.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation \n",
    "\n",
    "Now that we have train the model, lets use it to classify test images. The model will output a probability for each possible class. Numpys argmax is thus used to find the class with the highest probability and this class is used for the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10** Predict class from test set\n",
    "1. Load [HW9_test.csv](HW9_test.csv) using pandas, and **convert** the dataframe into tensorflow dataset using the **exact** pipeline you have used for training data\n",
    "* see **A2** and **A4** for data loading, reshaping, and scaling. \n",
    "2. Print the output for the first 15 images from tf_test_data below. \n",
    "3. Use numpy ``argmax`` to get the index with best classificaion score.\n",
    "\n",
    "**A10** Replace ??? with code in the code cell below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('HW9_test.csv')\n",
    "tf_test_data = tf.data.Dataset.from_tensor_slices(([test_data.to_numpy().reshape(len(test_data), 28, 28, 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(tf_test_data.take(15))\n",
    "predictions = np.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11** To see if the model is performing as expected I will check visualisations of the first fifteen images against the classifications produced by the model.\n",
    "1. Now, fetch first 15 rows from the test_data dataframe and plot the numeric arrays as images\n",
    "2. Use the predicted values from the above cell as their **xlabel**. Lets see how many of the labels are correct with the images.\n",
    "\n",
    "**A11** Replace ??? with code in the code cell below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for i, row in test_data.head(15).iterrows():\n",
    "    plt.subplot(3,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.imshow('xlabel'), cmap='gray')\n",
    "    plt.xlabel(predictions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12** Analyze the pipeline, the data, and the model output.  What is the accuracy on the validation set? Is it reflected in first 15 image analysis of the test set? \n",
    "\n",
    "**A12** Write your answer in the text cell below: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Reading\n",
    "    \n",
    "* [Neural Network](https://www.bmc.com/blogs/neural-network-introduction/)\n",
    "\n",
    "* [Convolutional Neural Network](https://towardsdatascience.com/covolutional-neural-network-cb0883dd6529)\n",
    "\n",
    "* [Tensorflow Framework](https://www.datacamp.com/community/tutorials/cnn-tensorflow-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of CS4347-Assignment2-NETID.ipynb",
   "provenance": [
    {
     "file_id": "1vsORsvIGVbJ2cNHR1lIPDIuGobP25ZPq",
     "timestamp": 1629391503766
    },
    {
     "file_id": "1vuQua73YBPg3xOVKXACAGdS_8R1OlQdX",
     "timestamp": 1611597429764
    },
    {
     "file_id": "1Jr8VoifAgTlPqVE_AiCDeWbiHGEyvkxq",
     "timestamp": 1580784119108
    }
   ]
  },
  "interpreter": {
   "hash": "61092feec5071c5645c5e9fd8e6ed751662eaf3fad39a8c2be0e7ae1843d914b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
